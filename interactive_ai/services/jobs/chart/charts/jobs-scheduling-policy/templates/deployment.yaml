apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Namespace }}-{{ .Chart.Name }}
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "jobs-scheduling-policy.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.replicas }}
  selector:
    matchLabels:
      {{- include "jobs-scheduling-policy.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "jobs-scheduling-policy.selectorLabels" . | nindent 8 }}
    spec:
      serviceAccountName: {{ .Release.Namespace }}-{{ .Chart.Name }}
      initContainers:
        # [mTLS STRICT] The kubectl-wait init container is used to wait for the init job that is related to this specific component.
        # This is a workaround to address the issue that arises when using Istio mTLS strict mode and init containers
        # that do not have the Istio proxy sidecar
        - name: "kubectl-wait"
          image: "{{ .Values.global.kubectl.registry }}/{{ if .Values.global.kubectl.repository }}{{ .Values.global.kubectl.repository }}/{{ end }}{{ .Values.global.kubectl.name }}"
          command: [ "/bin/bash", "-c" ]
          args:
            - >-
              kubectl wait jobs
              --timeout=1200s 
              --for=condition=complete 
              --namespace {{ .Release.Namespace }}
              init-job-{{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          resources:
            {{- toYaml .Values.initResources | nindent 12 }}
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          resources:
            {{ toYaml .Values.resources | nindent 12 }}
          image: "{{ .Values.global.registry_address }}/{{ .Values.global.docker_namespace }}/{{ .Values.image }}:{{ .Values.global.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.imagePullPolicy }}
          command:
            - python
            - policies/main.py
          env:
            - name: SCHEDULING_POLICY_SERVICE_LOOP_INTERVAL
              value: "1"
            - name: RESOURCE_MANAGER_LOOP_INTERVAL
              value: "60"
            - name: MAX_JOBS_RUNNING_PER_ORGANIZATION
              value: "1"
            - name: DATABASE_ADDRESS
              value: mongodb://{{ .Release.Namespace }}-mongodb:27017/
            - name: CREDITS_SERVICE
              value: credit-system.impt:5556
            {{- if .Values.global.enable_mongodb_credentials }}
            - name: DATABASE_USERNAME
              valueFrom:
                secretKeyRef:
                  name: {{ .Release.Namespace }}-mongodb
                  key: {{ .Chart.Name }}-mongodb-username
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ .Release.Namespace }}-mongodb
                  key: {{ .Chart.Name }}-mongodb-password
            {{- end }}
            - name: MONGODB_CREDENTIALS_PROVIDER
              valueFrom:
                configMapKeyRef:
                  name: {{ .Release.Namespace }}-configuration
                  key: mongodb_credentials_provider
            {{- if .Values.global.enable_object_storage }}
            - name: S3_CREDENTIALS_PROVIDER
              valueFrom:
                configMapKeyRef:
                  name: {{ .Release.Namespace }}-configuration
                  key: s3_credentials_provider
            {{- end }}
            - name: ENABLE_TRACING
              value: "true"
            - name: ENABLE_METRICS
              value: "true"
            - name: OTEL_SERVICE_NAME
              value: jobs-scheduling-policy
            - name: OTLP_TRACES_RECEIVER
              value: "{{ .Release.Namespace }}-opentelemetry-collector.{{ .Release.Namespace }}:4317"
            - name: OTLP_METRICS_RECEIVER
              value: "{{ .Release.Namespace }}-opentelemetry-collector.{{ .Release.Namespace }}:4317"
            - name: OTLP_TRACES_PROTOCOL
              value: grpc
            {{ toYaml .Values.policies | nindent 12 }}
          volumeMounts:
            - mountPath: /tmp
              name: temp
            - mountPath: {{ .Values.global.logging_config_dir }}
              name: config
              readOnly: true
          envFrom:
            - configMapRef:
                name: {{ .Release.Namespace }}-feature-flags
      volumes:
        - name: temp
          emptyDir: {}
        - name: config
          configMap:
            name: {{ .Release.Namespace }}-logging-config
